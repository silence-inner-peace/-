{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: 雷家寨 Age: 31.6138',\n",
       " 'Name: 分水岭 Age: 32.3663',\n",
       " 'Name: 毛田咀 Age: 11.4878',\n",
       " 'Name: 樟木村 Age: 40.3982',\n",
       " 'Name: 云丰村 Age: 46.1054',\n",
       " 'Name: 五丘田 Age: 49.1783',\n",
       " 'Name: 小沟 Age: 39.9625',\n",
       " 'Name: 巨耳沟 Age: 49.972',\n",
       " 'Name: 货郎冲 Age: 16.6083',\n",
       " 'Name: 跑马坪(1) Age: 36.4044']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# $example on:schema_inferring$\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"file:///trn/testdata/poihubei_all_100.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "places = parts.map(lambda p: Row(name = p[6], age = float(p[3])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaP = spark.createDataFrame(places)\n",
    "schemaP.createOrReplaceTempView(\"poihubei\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "poluted = spark.sql(\"SELECT name, age FROM poihubei WHERE age <= 50 AND age <= 100\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "polutedNames = poluted.rdd.map(lambda p: \"Name: \" + p.name + \" Age: \"+str(p.age))\n",
    "polutedNames.take(10)\n",
    "#for name in teenNames:\n",
    "#    print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
